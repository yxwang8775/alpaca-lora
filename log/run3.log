nohup: ignoring input
/home/wangyaoxiang/anaconda3/envs/lora/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /home/wangyaoxiang/anaconda3/envs/lora did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
/home/wangyaoxiang/anaconda3/envs/lora/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib')}
  warn(msg)

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/wangyaoxiang/anaconda3/envs/lora/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda112.so
CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so
CUDA SETUP: Highest compute capability among GPUs detected: 8.0
CUDA SETUP: Detected CUDA version 112
CUDA SETUP: Loading binary /home/wangyaoxiang/anaconda3/envs/lora/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda112.so...
Training Alpaca-LoRA model with params:
base_model: /nvme/wangyaoxiang/model/llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348
data_path: ./dataset_alpaca/dict_alpaca_gen.json
output_dir: /nvme/wangyaoxiang/model/lora-alpaca_gen
batch_size: 256
micro_batch_size: 4
num_epochs: 3
learning_rate: 0.0003
cutoff_len: 256
val_set_size: 2000
lora_r: 8
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules: ['q_proj', 'v_proj']
train_on_inputs: True
add_eos_token: False
group_by_length: False
wandb_project: 
wandb_run_name: 
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt template: alpaca

Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:11,  2.75it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:10,  2.82it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:10,  2.75it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:10,  2.71it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:01<00:10,  2.68it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:10,  2.66it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:02<00:09,  2.71it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:02<00:09,  2.75it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:03<00:08,  2.84it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:03<00:07,  3.22it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:03<00:07,  3.01it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:04<00:07,  2.85it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:04<00:06,  2.90it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:04<00:06,  2.85it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:05<00:06,  2.78it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:05<00:06,  2.77it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:06<00:05,  2.75it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:06<00:05,  2.69it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:06<00:05,  2.64it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:07<00:04,  2.68it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:07<00:04,  2.69it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:08<00:04,  2.50it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:08<00:03,  2.51it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:08<00:03,  2.49it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:09<00:03,  2.53it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:09<00:02,  2.58it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:10<00:03,  1.58it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:11<00:02,  1.79it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:11<00:02,  1.98it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:11<00:01,  2.15it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:12<00:00,  2.28it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:12<00:00,  2.39it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:12<00:00,  2.70it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:12<00:00,  2.55it/s]
Downloading and preparing dataset json/default to /home/wangyaoxiang/.cache/huggingface/datasets/json/default-57404d199f47d8d7/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...
Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 8701.88it/s]
Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 1092.55it/s]
Generating train split: 0 examples [00:00, ? examples/s]Failed to read file '/home/wangyaoxiang/codes/alpaca-lora/dataset_alpaca/dict_alpaca_gen.json' with error <class 'pyarrow.lib.ArrowInvalid'>: JSON parse error: Column() changed from object to string in row 0
Generating train split: 0 examples [00:00, ? examples/s]                                                        Traceback (most recent call last):
  File "/home/wangyaoxiang/anaconda3/envs/lora/lib/python3.10/site-packages/datasets/builder.py", line 1858, in _prepare_split_single
    for _, table in generator:
  File "/home/wangyaoxiang/anaconda3/envs/lora/lib/python3.10/site-packages/datasets/packaged_modules/json/json.py", line 151, in _generate_tables
    raise ValueError(
ValueError: Not able to read records in the JSON file at /home/wangyaoxiang/codes/alpaca-lora/dataset_alpaca/dict_alpaca_gen.json. You should probably indicate the field of the JSON file containing your records. This JSON file contain the following fields: ['train']. Select the correct one and provide it as `field='XXX'` to the dataset loading method. 

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/wangyaoxiang/codes/alpaca-lora/finetune.py", line 284, in <module>
    fire.Fire(train)
  File "/home/wangyaoxiang/anaconda3/envs/lora/lib/python3.10/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/home/wangyaoxiang/anaconda3/envs/lora/lib/python3.10/site-packages/fire/core.py", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File "/home/wangyaoxiang/anaconda3/envs/lora/lib/python3.10/site-packages/fire/core.py", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "/home/wangyaoxiang/codes/alpaca-lora/finetune.py", line 188, in train
    data = load_dataset("json", data_files=data_path)
  File "/home/wangyaoxiang/anaconda3/envs/lora/lib/python3.10/site-packages/datasets/load.py", line 1797, in load_dataset
    builder_instance.download_and_prepare(
  File "/home/wangyaoxiang/anaconda3/envs/lora/lib/python3.10/site-packages/datasets/builder.py", line 890, in download_and_prepare
    self._download_and_prepare(
  File "/home/wangyaoxiang/anaconda3/envs/lora/lib/python3.10/site-packages/datasets/builder.py", line 985, in _download_and_prepare
    self._prepare_split(split_generator, **prepare_split_kwargs)
  File "/home/wangyaoxiang/anaconda3/envs/lora/lib/python3.10/site-packages/datasets/builder.py", line 1746, in _prepare_split
    for job_id, done, content in self._prepare_split_single(
  File "/home/wangyaoxiang/anaconda3/envs/lora/lib/python3.10/site-packages/datasets/builder.py", line 1891, in _prepare_split_single
    raise DatasetGenerationError("An error occurred while generating the dataset") from e
datasets.builder.DatasetGenerationError: An error occurred while generating the dataset
